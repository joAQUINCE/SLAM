{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import built in modules\n",
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import zipfile as zf\n",
    "\n",
    "!pip install bresenham\n",
    "from bresenham import bresenham\n",
    "import pandas as pd\n",
    "import time \n",
    "# !pip install tqdm -U --user\n",
    "!pip install tqdm\n",
    "# import tqdm\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from scipy.stats import norm as norm_rv\n",
    "from scipy.stats import rv_discrete \n",
    "from matplotlib.animation import FuncAnimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "''' NOTE: This code will be used when running code on AWS, to unzip file'''\n",
    "# files = zf.ZipFile(\"ec833_project3_odom.zip\", 'r')\n",
    "# files.extractall('')\n",
    "# files.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "''' NOTE: Must NOT separate custom module imports from raw data import '''\n",
    "\n",
    "# import custom modules + data\n",
    "\n",
    "import load_data, dataloader\n",
    "\n",
    "custom_modules = [\"load_data\",\"dataloader\"]\n",
    "\n",
    "# DELETE: assures up-to-date local modules are always used\n",
    "for module_par in custom_modules:\n",
    "    del sys.modules[module_par]\n",
    "\n",
    "# RELOAD: assures up-to-date local modules are always used\n",
    "import load_data, dataloader\n",
    "\n",
    "\n",
    "# load all data from target folder\n",
    "data_folder = \"data\"\n",
    "\n",
    "# produces list with objects of class \"map_object\" in it\n",
    "map_list_train = list(dataloader.load_folder(data_folder))\n",
    "\n",
    "for map_number in range(len(map_list_train)):\n",
    "    \n",
    "    print(\"Map ID\",map_list_train[map_number].id)\n",
    "    print(\"Units are Meters. Robot is assumed to start facing right at coordinate (0,0)\")\n",
    "    fig1 = plt.figure(figsize=(15,15))\n",
    "    plt.plot(map_list_train[map_number].cummulative_displacement[:,0],\\\n",
    "             map_list_train[map_number].cummulative_displacement[:,1])\n",
    "    plt.xlabel('Robot Horizontal Location (meters)', fontsize=18)\n",
    "    plt.ylabel('Robot Vertical Location (meters)', fontsize=18)\n",
    "    plt.xticks(fontsize= 16)\n",
    "    plt.yticks(fontsize= 16) \n",
    "\n",
    "    fig1.suptitle('Odometry Navigation Map {0}'.format(map_list_train[map_number].id), fontsize=20)\n",
    "    fig1.tight_layout()\n",
    "    fig1.subplots_adjust(top=0.95)\n",
    "    fig1.savefig('maps/odometry_map{0}.png'.format(map_list_train[map_number].id))\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "top = '\\n##########################################################'\n",
    "bot = \"##########################################################\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_effective_n(weights_current):\n",
    "    return 1 / np.sum(weights_current**2)\n",
    "    \n",
    "     #how to test this:\n",
    "    '''a = np.array([[4],[6]])\n",
    "    print(a**2)'''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def normalize_weights(weights_resampled_par):\n",
    "    weights_resampled_par /= np.sum(weights_resampled_par)\n",
    "    return weights_resampled_par\n",
    "\n",
    "    #how to test this:\n",
    "    ''' weights_test = np.ones(10)/10\n",
    "        weights_test /= 2\n",
    "        print((weights_test[0]))'''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def resample_particles(weights_current,particles_par):\n",
    "    \n",
    "    # get the number of weights\n",
    "    weight_cnt = weights_current.shape[0]\n",
    "    \n",
    "    weights_cum = np.asarray([np.sum(weights_current[0:i+1]) for i in range(weight_cnt)])\n",
    "    \n",
    "    # produce n uniform random samples\n",
    "    random_density_samples = np.random.rand(weight_cnt)\n",
    "    \n",
    "    list_add = []\n",
    "    for sample in random_density_samples:\n",
    "        list_add.append(np.sum(weights_cum < sample))\n",
    "       \n",
    "    randomly_sampled_indexes = np.asarray(list_add)\n",
    "    \n",
    "    weights_resampled  = weights_current[randomly_sampled_indexes]\n",
    "    weights_resampled = normalize_weights(weights_resampled)\n",
    "    \n",
    "    particle_resampled = particles_par[randomly_sampled_indexes]\n",
    "\n",
    "    return particle_resampled,weights_resampled\n",
    "    \n",
    "    #how to test this:\n",
    "    '''print(np.argsort(weights_current),\"np.argsort(weights_current)\")\n",
    "    print(np.argsort(np.bincount(randomly_sampled_indexes)),\"np.argsort(np.bincount(randomly_sampled_indexes))\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_particles(x0, y0, theta0, x_noise, y_noise, theta_noise):\n",
    "    \n",
    "    # create a single 10 x 10 figure\n",
    "    fig_particles, plt_particles = plt.subplots(figsize=(10,10))\n",
    "    \n",
    "    # print \n",
    "    title_particles = str(\"pose_angle: \"+str(pose_angle_old_SLAM*180/np.pi)+\\\n",
    "                          \" | d_theta from ODOMETRY \"+str(d_theta*180/np.pi)+\"| dx from ODOMETRY \"+str(dx)+\\\n",
    "                          \" dy from ODOMETRY\"+ str(dy))\n",
    "    fig_particles.suptitle(title_particles, fontsize=16)\n",
    "\n",
    "    \n",
    "    plt_particles.set_xlabel('distance (PIXELS)')\n",
    "    plt_particles.set_ylabel('distance (PIXELS)')\n",
    "\n",
    "    plt_particles.quiver(x0,y0,np.cos(theta0),np.sin(theta0))\n",
    "    plt_particles.quiver(x_noise,y_noise,np.cos(theta_noise),np.sin(theta_noise), color='m', alpha=1, headwidth=.3*conv_factor)\n",
    "    \n",
    "    plt_particles.ticklabel_format(useOffset=False)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    #     fig_particles.show()\n",
    "    print(\"TIME STEP FOR PLOT PARTICLE ABOVE:\",time_step)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def make_lidar_rays(lidar_global_hit_locs_par,particles_par):\n",
    "    \n",
    "    # extract the 0th column of all particles, which represents the angles\n",
    "    angle_matrix = particles_par[:,0]\n",
    "    \n",
    "    # make an array where each column is the consine and sine of the angle associated with each particle\n",
    "    transform_matrix = np.asarray([np.cos(angle_matrix),np.sin(angle_matrix)])\n",
    "    \n",
    "    '''lidar_global_hit_locs_par contains the rotation matrix corresponding to each ray (1080 of them) in its local\n",
    "    coordinate frame (angles from -135 to 135). The dot product below computes the transformation to the local frame'''\n",
    "    global_lidar_vecs_matrix = np.dot(lidar_global_hit_locs_par,transform_matrix)\n",
    "    #print(global_lidar_vecs_matrix.shape) # (1810 x 2 by number of particles)\n",
    "    return global_lidar_vecs_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def update_map(points_impacted_par,map_array_par):\n",
    "    map_array_update = np.copy(map_array_par)\n",
    "    if map_array_update[points_impacted_par[-1]]<100:\n",
    "            map_array_update[points_impacted_par[-1]] += 10    #* (angle_factor)\n",
    "    \n",
    "    for i in points_impacted_par[0:-1]:\n",
    "        if map_array_update[i] > -100:\n",
    "            map_array_update[i] += -.5   \n",
    "    return map_array_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def make_noise(dx,dy,d_theta,particle_cnt):\n",
    "\n",
    "    # make numbers between -1 and 1\n",
    "    noise_x = 2 * (np.random.rand(particle_cnt)-0.5)\n",
    "    # scale ONLY RANDOM noise by conversion factor \n",
    "    random_noise_x = x_random_noise_factor * noise_x * conv_factor\n",
    "    # recalculate percent noise based on GLOBAL factor\n",
    "    noise_x = factor_x * noise_x \n",
    "\n",
    "    # make numbers between -1 and 1\n",
    "    noise_y = 2 * (np.random.rand(particle_cnt)-0.5)\n",
    "    # scale ONLY RANDOM noise by conversion factor \n",
    "    random_noise_y =   y_random_noise_factor* noise_y * conv_factor\n",
    "    # recalculate percent noise based on GLOBAL factor\n",
    "    noise_y = factor_y * noise_y\n",
    "\n",
    "    # make numbers between -1 and 1\n",
    "    noise_theta = 2 * (np.random.rand(particle_cnt)-0.5)\n",
    "    # scale ONLY RANDOM noise by conversion factor \n",
    "    random_noise_theta = theta_random_noise_factor * noise_theta\n",
    "    # recalculate percent noise based on GLOBAL factor\n",
    "    noise_theta = factor_theta * noise_theta            \n",
    "\n",
    "    noised_d_x     = dx * (1 + noise_x)          + random_noise_x \n",
    "    noised_d_y     = dy * (1 + noise_y)          + random_noise_y \n",
    "    noised_d_theta = d_theta * (1 + noise_theta) + random_noise_theta \n",
    "\n",
    "    min_dx = np.amin(noised_d_x)\n",
    "    max_dx = np.amax(noised_d_x)\n",
    "\n",
    "    min_dy = np.amin(noised_d_y)\n",
    "    max_dy = np.amax(noised_d_y)\n",
    "\n",
    "    min_d_theta = np.amin(noised_d_theta)\n",
    "    max_d_theta = np.amax(noised_d_theta)\n",
    "\n",
    "    return noised_d_x, noised_d_y, noised_d_theta,\\\n",
    "           min_dx, min_dy, min_d_theta,\\\n",
    "           max_dx, max_dy, max_d_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([4,5,5,3])\n",
    "print(np.unique(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(1080/2-10,1080/2+12,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# number of particles for model\n",
    "particle_cnt = 100\n",
    "\n",
    "# threhold for robot-body lidar noise, in meters, compared with lidar readings\n",
    "robot_self_dist = .33\n",
    "\n",
    "# +/- angle (with respect to straight ahead direaction) that is not expected to result in robot-body lidar noise\n",
    "angle_reducer = 90\n",
    "\n",
    "# pixels per meter\n",
    "conv_factor = 10\n",
    "\n",
    "# wdith of map in meters\n",
    "map_width = 100\n",
    "\n",
    "# holds lists of arrays, which are used to produce plots\n",
    "map_plot_list = []\n",
    "\n",
    "\n",
    "# hold list of lidar parameters\n",
    "map_lidar_list = []\n",
    "\n",
    "# hold numpy arrays of all maps\n",
    "all_map_list = []\n",
    "\n",
    "# number of (particle / map) plots allowed\n",
    "particle_plot_cnt_allowed = 300\n",
    "\n",
    "# numeric parameter of width of current map in pixels, will be a function parameter at the end \n",
    "map_cm = map_width * conv_factor\n",
    "map_cm = map_cm + 1\n",
    "\n",
    "# if the map width is not divisible by 2, issue warning\n",
    "if ((map_cm - 1)%2) > 0:\n",
    "    print (\"ISSUE WITH THE MAP DIMENSION!\") \n",
    "\n",
    "# center in x and y\n",
    "x_center = int((map_cm - 1)/2)\n",
    "y_center = np.copy(x_center)\n",
    "\n",
    "# start plotting information at this time step\n",
    "start_time_master = 1700\n",
    "\n",
    "# complete this number of steps before stopping plotting\n",
    "time_steps_master = 5000\n",
    "\n",
    "# number of time steps between plots\n",
    "plot_interval = 3\n",
    "\n",
    "# hyperparameter save file parameter \n",
    "saved = 0\n",
    "\n",
    "'''number from 1 to 10, representing the size of each random sample for angles at each time step\n",
    "in multiples of 108'''\n",
    "downsample = 5\n",
    "\n",
    "# number of angles sampled for every time step\n",
    "random_sample_cnt = 108*downsample\n",
    "\n",
    "# iterate through all the maps\n",
    "for idx in range(len(map_list_train)):\n",
    "    \n",
    "    # capture start time\n",
    "    start = time.time()\n",
    "    \n",
    "    plot_array_list = []\n",
    "    # initialize particle weights as *** 1/number of particles\n",
    "    weights = np.ones(particle_cnt)/particle_cnt \n",
    "    \n",
    "    # initialize weights old with the same uniform values\n",
    "    weights_old = np.copy(weights)\n",
    "    \n",
    "    # print map id from filename\n",
    "    print(\"Map ID:\",map_list_train[idx].id)\n",
    "    \n",
    "#     if int(map_list_train[idx].id) != 24 and int(map_list_train[idx].id) != 23:\n",
    "#         continue\n",
    "    \n",
    "#      if map is not 23,skip (for testing only)\n",
    "#     if int(map_list_train[idx].id) == 23:\n",
    "#         continue\n",
    "\n",
    "#     if int(map_list_train[idx].id) != 20:\n",
    "#         continue\n",
    "\n",
    "#     if map is not 21,skip (for testing only)\n",
    "#     if int(map_list_train[idx].id) != 21:\n",
    "#         continue\n",
    "\n",
    "#     # if map is not 20,skip (for testing only)\n",
    "#     if int(map_list_train[idx].id) == 20:\n",
    "#         continue\n",
    "        \n",
    "        \n",
    "    # list to hold IMU data lined up with encoder and lidar data\n",
    "#     acceleration_list = []\n",
    "    \n",
    "    \n",
    "    print(\"Execution Started\")\n",
    "\n",
    "    # map nummpy array (to be populated)\n",
    "    map_array = np.zeros([map_cm,map_cm])    \n",
    "\n",
    "    # times, angles and ray return distances from a map id 'idx' \n",
    "    lidar_data = map_list_train[idx].lidar\n",
    "\n",
    "    # Number of time observations for this map\n",
    "    time_ob_cnt   = len(lidar_data)\n",
    "\n",
    "    # time index where we start aligning lidar and encoder using a subsampled version of the encoder time array\n",
    "    range_start = 3\n",
    "\n",
    "    time_match_list = []\n",
    "\n",
    "    ''' meas_per_tstep: get number of lidar measurements for every time step.\n",
    "         lidar_angle_vec: get lidar angle vector array (same angles spanned for \n",
    "         all measurements, hence 17 hardcoded)\n",
    "\n",
    "    17 is safely HARDCODED here because, no matter how many time steps there are, \n",
    "    there will be many more than 17 (picked that number just 'because') \n",
    "    all the measurements will have the same number of angular lidar measurements (1801)\n",
    "    but didn't want to hardcode 1801, just in case.'''\n",
    "    \n",
    "    # lidar angle vector is from -135 to 135 ********in RADIANS*******\n",
    "    meas_per_tstep  ,   lidar_angle_vec =   lidar_data[17]['angle'].shape[0]  ,    lidar_data[17]['angle']\n",
    "    \n",
    "    # make list of transformations for local to global coordinate frames\n",
    "    ray_transform_vec =np.asarray([ [[np.cos(lidar_angle_vec[i]), -np.sin(lidar_angle_vec[i])],                                     [np.sin(lidar_angle_vec[i]),  np.cos(lidar_angle_vec[i])]]                                      for i in range(meas_per_tstep)]) \n",
    "    # transform list to array\n",
    "    ray_transform_vec = np.squeeze(ray_transform_vec)\n",
    "    \n",
    "    '''print(ray_transform_vec[540+180])\n",
    "       TESTED EQUALS [[ 0.70710678 -0.70710678]\n",
    "                      [ 0.70710678  0.70710678]]'''\n",
    "    \n",
    "    #compute lidar off-center distance from wheelbase\n",
    "    wheelbase_len   = 0.33020\n",
    "    \n",
    "    # locate distance of lidar lever from center of robot\n",
    "    lidar_lever_len = 0.29833 - (wheelbase_len/2)\n",
    "    \n",
    "    # convert to small unit\n",
    "    lidar_lever_len = lidar_lever_len * conv_factor\n",
    "\n",
    "    # initialize old float (prior step) position for encoder\n",
    "    x0_encoder_float_old = np.copy(x_center)\n",
    "    y0_encoder_float_old = np.copy(y_center)\n",
    "    \n",
    "    # initialize old pose angle for encoder\n",
    "    pose_angle_old = 0\n",
    "    pose_angle_old_SLAM = 0\n",
    " \n",
    "    # initialize old float (prior step) position for lidar basepoint\n",
    "    x0_lidar_float_old = x_center + lidar_lever_len*np.cos(pose_angle_old)\n",
    "    y0_lidar_float_old = y_center + lidar_lever_len*np.sin(pose_angle_old)\n",
    "\n",
    "    # initialize old int (prior step) position for lidar basepoint. this is fine since the lidar\n",
    "    # will hit a point different from 0,0 as soon as it starts running\n",
    "    x1_lidar_int_old = 0\n",
    "    y1_lidar_int_old = 0\n",
    "\n",
    "    # counter of particle plots, to limit the number of plots generated at runtime\n",
    "    particle_plot_cnt = 0\n",
    "    \n",
    "    # an array holding the ENCODER time values for the current map as indexed by idx\n",
    "    time_array_encoder = map_list_train[idx].processed_encoder[:,0]\n",
    "    \n",
    "    # the IMU time for the current map as indexed by idx\n",
    "    imu_time = np.transpose(map_list_train[idx].imu)[:,6]\n",
    "    \n",
    "    # the IMU data for the current map as indexed by idx\n",
    "    imu_data = np.transpose(map_list_train[idx].imu)[:,:6]\n",
    "    \n",
    "    # random numbers for particle coloring, each color is 3 channels\n",
    "    colors_rays = tuple(np.random.rand(particle_cnt,3))\n",
    "    color_center = tuple(np.random.rand(3))\n",
    "    \n",
    "    list_test=[]\n",
    "    \n",
    "    displacements = map_list_train[idx].cummulative_displacement\n",
    "    \n",
    "    for time_step in tqdm(range(time_ob_cnt)):\n",
    "        \n",
    "        ## if we are testing, use this to stop after a certain number of time steps\n",
    "#         if time_step > time_steps_master + start_time_master:\n",
    "#             break\n",
    "        \n",
    "        # boolean variable: allow print if not too many plots have been generated \n",
    "        allow_print = False #particle_plot_cnt < particle_plot_cnt_allowed and time_step >= start_time_master and time_step % plot_interval == 0\n",
    "        \n",
    "        # list of hits for a time step based on the x,y and angle prediction\n",
    "        hit_list = []\n",
    "        \n",
    "        # if time is less than a small integer, find the smallest difference between the encoder time array \n",
    "        # and current lidar time, return the index   \n",
    "        if time_step < range_start:\n",
    "            lidar_time_closest_arg = (np.abs(time_array_encoder - map_list_train[idx].lidar[time_step]['t'])).argmin()\n",
    "        \n",
    "        # if time is larger, find the smallest difference between as smaller version (size is determined by range_start) \n",
    "        # of the encoder time array and the current lidar time, return the index\n",
    "        else:\n",
    "            lidar_time_closest_small = time_array_encoder[lidar_time_closest_arg:lidar_time_closest_arg+range_start]\n",
    "            lidar_time_closest_arg = (np.abs(lidar_time_closest_small - map_list_train[idx].lidar[time_step]['t'])).argmin() + lidar_time_closest_arg\n",
    "\n",
    "        # robot pose angle in RADIANS\n",
    "        pose_angle = map_list_train[idx].cummulative_angle[lidar_time_closest_arg]\n",
    "        \n",
    "#     TESTED: JUST COMMENT ALL BELOW THESE LIST/TWO PLOT LINES BELOW\n",
    "#         list_test.append(pose_angle)\n",
    "#     plt.plot(np.asarray(list_test)*180/np.pi)\n",
    "#     plt.plot(np.zeros(len(list_test)))\n",
    "#     plt.show()\n",
    "\n",
    "     # get x coordinate of robot, initially based on encoder\n",
    "        x0_encoder_float = displacements[lidar_time_closest_arg,0]\n",
    "\n",
    "        # convert to small unit (i.e.: centimeters), converted  \n",
    "        x0_encoder_float = x0_encoder_float * conv_factor\n",
    "\n",
    "        # account for scaling center shift, converted \n",
    "        x0_encoder_float = x0_encoder_float + x_center\n",
    "                \n",
    "        # transfer to centroid of lidar, account for lidar being off center with encoder, BOTH IN SMALL UNITS\n",
    "        x0_lidar_float = x0_encoder_float + lidar_lever_len*np.cos(pose_angle) \n",
    "\n",
    "        # get y coordinate of robot, initially based on encoder\n",
    "        y0_encoder_float = displacements[lidar_time_closest_arg,1]\n",
    "\n",
    "        # convert to small unit (i.e.: centimeters)    \n",
    "        y0_encoder_float = y0_encoder_float * conv_factor\n",
    "\n",
    "        # account for scaling center shift\n",
    "        y0_encoder_float = y0_encoder_float + y_center    \n",
    "        \n",
    "        # transfer to centroid of lidar, account for lidar being off center with encoder\n",
    "        y0_lidar_float= y0_encoder_float + lidar_lever_len*np.sin(pose_angle)\n",
    "        \n",
    "        x0_lidar_int = int(np.around(x0_lidar_float))\n",
    "        y0_lidar_int = int(np.around(y0_lidar_float))\n",
    "        \n",
    "        # measure distance travelled in time step\n",
    "        distance_traveled = np.linalg.norm(np.array([[y0_lidar_float-y0_lidar_float_old],[x0_lidar_float-x0_lidar_float_old]]))\n",
    "\n",
    "        '''if distance travelled is less than one micrometer, \n",
    "        AND change is heading angle is less than .01 degree(s) ---> cell updates based on previous reading'''\n",
    "        if time_step != 0 and distance_traveled < conv_factor * .00001 and abs(pose_angle-pose_angle_old)<np.pi/(18000): \n",
    "            map_array = update_map(points_impacted,map_array)\n",
    "            continue\n",
    "\n",
    "        factor_x = .8/20\n",
    "        factor_y = .1/10 #.35\n",
    "        factor_theta = 3/20\n",
    "        \n",
    "        #aqui\n",
    "        x_random_noise_factor = 0.03072 \n",
    "        x_random_noise_factor = x_random_noise_factor*12\n",
    "          \n",
    "        y_random_noise_factor = 0.025\n",
    "        y_random_noise_factor = y_random_noise_factor /.65\n",
    "        \n",
    "        theta_random_noise_factor = 0.012\n",
    "        theta_random_noise_factor = theta_random_noise_factor*6\n",
    "\n",
    "        d_theta = (pose_angle - pose_angle_old)                    \n",
    "        dx      = (x0_encoder_float - x0_encoder_float_old)\n",
    "        dy      = (y0_encoder_float - y0_encoder_float_old)\n",
    "        \n",
    "        noised_d_x, noised_d_y, noised_d_theta,        min_dx,min_dy,min_d_theta,      \\\n",
    "        max_dx,max_dy,max_d_theta = make_noise(dx,dy,d_theta,particle_cnt)\n",
    "        \n",
    "        if saved == 0:\n",
    "            saved = 1\n",
    "            hyperparameters = [\"random_sample_cnt \", str(random_sample_cnt),\"\\n\",\\\n",
    "                               \"particle_cnt \",str(particle_cnt),\"\\n\",\\\n",
    "                               \"conv_factor \",str(conv_factor),\"\\n\",\\\n",
    "                               \"factor_x \",str(factor_x),\"\\n\",\\\n",
    "                               \"factor_y \",str(factor_y),\"\\n\",\\\n",
    "                               \"factor_theta \", str(factor_theta),\"\\n\",\\\n",
    "                               \"x_random_noise_factor \", str(x_random_noise_factor),\"\\n\",\\\n",
    "                               \"y_random_noise_factor \", str(y_random_noise_factor),\"\\n\",\\\n",
    "                               \"theta_random_noise_factor \", str(theta_random_noise_factor)]\n",
    "            \n",
    "            hyper_filename = \"hyper_parameters/hyperparameters_for_map_\"+str(map_list_train[idx].id)+\"_time_\"+str(start)+\".txt\"\n",
    "            with open (hyper_filename, 'w') as file:\n",
    "                for i in hyperparameters:\n",
    "                    file.write(i)\n",
    "                file.close()\n",
    "            \n",
    "#             hyperparameters = np.savetxt(hyper_filename,np.asarray(hyperparameters))\n",
    "            \n",
    "        if get_effective_n(weights) < particle_cnt * .5:\n",
    "#         if get_effective_n(weights) < 15:     \n",
    "#             print(\"resampling particles. Effective n:\", get_effective_n(weights),\"Time Step\",time_step,\"\\n\",  )\n",
    "#             print(np.argsort(weights))\n",
    "            particles,weights_old = resample_particles(weights,particles)\n",
    "        \n",
    "        if time_step == 0:\n",
    "            pt_no_noise = pose_angle # store old ANGLE\n",
    "            px_no_noise = x0_encoder_float # store old x encoder origin\n",
    "            py_no_noise = y0_encoder_float # store old y encoder origin\n",
    "            \n",
    "            theta_particles = pose_angle       + noised_d_theta # update encoder angle with noise\n",
    "            x_particles     = x0_encoder_float + noised_d_x     # udpate encoder x with noise\n",
    "            y_particles     = y0_encoder_float + noised_d_y     # update encoder y with noise \n",
    " \n",
    "        if time_step >0:  \n",
    "            # parameters in particles before any new displacements or angle changes are added\n",
    "            pt_no_noise = particles[:,0] # store particles before noise, ANGLE\n",
    "            px_no_noise = particles[:,1] # store old x encoder origin for particles\n",
    "            py_no_noise = particles[:,2] # store old y encoder origin for particles\n",
    "            \n",
    "            # parameters in particles after new displacements or angle changes are added\n",
    "            theta_particles = particles[:,0] + noised_d_theta # update encoder angle with noise\n",
    "            x_particles     = particles[:,1] + noised_d_x     # update encoder x with noise\n",
    "            y_particles     = particles[:,2] + noised_d_y     # update encoder y with noise\n",
    "            \n",
    "        # score the particle data in \"particles variable\"\n",
    "        particles = np.hstack((theta_particles[:,np.newaxis],x_particles[:,np.newaxis],y_particles[:,np.newaxis]))\n",
    "\n",
    "        if False and allow_print: # if printing is allowed, pring the particles.\n",
    "            particle_plot_cnt +=1\n",
    "            plot_particles(px_no_noise,  py_no_noise  , pt_no_noise,\\\n",
    "                           particles[:,1], particles[:,2], particles[:,0])\n",
    "\n",
    "        '''update \"old_encoder\", \"old lidar\" and \"old_pose angle\". ''' \n",
    "        x0_encoder_float_old = np.copy(x0_encoder_float)\n",
    "        y0_encoder_float_old = np.copy(y0_encoder_float)\n",
    "        \n",
    "        x0_lidar_float_old   = np.copy(x0_lidar_float)\n",
    "        y0_lidar_float_old   = np.copy(y0_lidar_float)\n",
    "        \n",
    "        pose_angle_old       = np.copy(pose_angle)\n",
    "        pose_angle_old_SLAM  = np.copy(pose_angle_old)\n",
    "        \n",
    "        # update the scan vector based on current time step, IN SMALL UNITS!\n",
    "        lidar_scan_vec  = lidar_data[time_step]['scan'] * conv_factor\n",
    "\n",
    "        '''At current time step, multiplies the hits (distances in meters) for every angle  \n",
    "        by the corresponding local (2x2) transform for that angle'''    \n",
    "        # returns scaled cosine and sine components of reading along the robot's local coordinate frame, \n",
    "        # arranged in 2x2 matrices within a list, ready to transfer to global frame \n",
    "        \n",
    "        '''lidar_scan_vec holds the readings from the lidar from angles -135 to 135\n",
    "        ray_transform_vec holds the transformation matrix at each of those angles'''\n",
    "        lidar_global_hit_locs = [lidar_scan_vec[i]*ray_transform_vec[i] for i in range(meas_per_tstep)]\n",
    "\n",
    "        # converts this list reading to numpy array, in meters\n",
    "        lidar_global_hit_locs = np.asarray(lidar_global_hit_locs)\n",
    "\n",
    "        # adds offset to x lidar distance reading (based on the lidar off-center mounting distance), in meters\n",
    "        lidar_global_hit_locs = lidar_global_hit_locs +  np.array([[lidar_lever_len,0],[0,lidar_lever_len]])\n",
    "               \n",
    "        # converts to array and squeeze extra dimension\n",
    "        lidar_global_hit_locs = np.asarray(lidar_global_hit_locs)\n",
    "        lidar_global_hit_locs = np.squeeze(lidar_global_hit_locs)\n",
    "#         print(lidar_global_hit_locs.shape,\"        print(lidar_global_hit_locs.shape)\")\n",
    "        \n",
    "        '''A REDUCED MATRIX IS INTRODUCED HERE'''\n",
    "        \n",
    "        ray_indexer = np.random.choice(meas_per_tstep, random_sample_cnt)\n",
    "        ray_indexer = np.hstack((ray_indexer,np.arange(1080/2-20,1080/2+20,1).astype(int)))\n",
    "# #         print(ray_indexer)\n",
    "        ray_indexer = np.unique(ray_indexer)\n",
    "#         print(len(ray_indexer),\"len\")\n",
    "        \n",
    "        lidar_global_hit_locs_reduced = lidar_global_hit_locs[ray_indexer]\n",
    "        \n",
    "        if time_step > 1:\n",
    "\n",
    "            # transfer all lidar rays for all particles to local frame\n",
    "            lidar_ray_matrix_reduced = make_lidar_rays(lidar_global_hit_locs_reduced,particles)\n",
    "            \n",
    "            \n",
    "            # for each time step, make the particle score zero to begin with\n",
    "            particle_score = np.zeros(particle_cnt)\n",
    "            \n",
    "            \n",
    "            for reading ,ray in enumerate(lidar_ray_matrix_reduced):\n",
    "                ''' for this given time step and angle, distance is less than 0.33 meters and abs(angle) > 90 degrees, SKIP THIS READING'''\n",
    "                if lidar_data[time_step]['scan'][reading] < robot_self_dist and (abs(180/np.pi*lidar_angle_vec[reading])> angle_reducer):\n",
    "                    continue          \n",
    "                \n",
    "                #for each of the 1081 readings, set the x and y endpoints to ray values plus the center of each particle\n",
    "                x1_rays_float = ray[0,:]  + particles[:,1]\n",
    "                y1_rays_float = ray[1,:]  + particles[:,2]\n",
    "\n",
    "                # make these vectors integers\n",
    "                x1_rays_int = np.around(x1_rays_float).astype(int)\n",
    "                y1_rays_int = np.around(y1_rays_float).astype(int)\n",
    "\n",
    "#                 particle_score += np.multiply(map_array[y1_rays_int,x1_rays_int]>0,map_array[y1_rays_int,x1_rays_int])\n",
    "                particle_score += map_array[y1_rays_int,x1_rays_int]\n",
    "\n",
    "            shift = np.amax(particle_score)\n",
    "            weights = np.exp(particle_score-shift)\n",
    "            weights = weights/np.sum(weights)\n",
    "            weights = np.multiply(weights,weights_old)\n",
    "            weights = normalize_weights(weights)\n",
    "\n",
    "            weights_old = np.copy(weights)\n",
    "            \n",
    "            index_max = np.argmax(particle_score)\n",
    "            pose_angle       =  particles[index_max,0]\n",
    "            x0_encoder_float =  particles[index_max,1]\n",
    "            y0_encoder_float =  particles[index_max,2]\n",
    "\n",
    "            if allow_print:\n",
    "            \n",
    "                print(\"\\n####################################################\")\n",
    "                print(\"Odometry dx\", dx,\"for time\", time_step)\n",
    "                print(\"Minimum dx\", min_dx,\"for time\", time_step)\n",
    "                print(\"Maximum dx\", max_dx,\"for time\", time_step)\n",
    "                print(\"Choosen dx\", noised_d_x[index_max],\"for time\", time_step,\"\\n\")\n",
    "                \n",
    "                print(\"Odometry dy\", dy,\"for time\", time_step)\n",
    "                print(\"Minimum dy\", min_dy,\"for time\", time_step)\n",
    "                print(\"Maximum dy\", max_dy,\"for time\", time_step)\n",
    "                print(\"Choosen dy\", noised_d_y[index_max],\"for time\", time_step,\"\\n\")\n",
    "                \n",
    "                print(\"Odometry d_theta\", d_theta,\"for time\", time_step)\n",
    "                print(\"Minimum d_theta\", min_d_theta,\"for time\", time_step)\n",
    "                print(\"Maximum d_theta\", max_d_theta,\"for time\", time_step)\n",
    "                print(\"Choosen d_theta\", noised_d_theta[index_max],\"for time\", time_step, \"\\n\")\n",
    "                \n",
    "                print(\"####################################################\\n\")\n",
    "\n",
    "            pose_angle_old_SLAM = np.copy(pose_angle)\n",
    " \n",
    "            # transfer to centroid of lidar, account for lidar being off center with encoder, converted\n",
    "            x0_lidar_float = x0_encoder_float + lidar_lever_len*np.cos(pose_angle) \n",
    "\n",
    "            # transfer to centroid of lidar, account for lidar being off center with encoder, converted\n",
    "            y0_lidar_float = y0_encoder_float + lidar_lever_len*np.sin(pose_angle) \n",
    "            \n",
    "            # convert all origin and end point to int variables\n",
    "            x0_lidar_int = int(np.around(x0_lidar_float))\n",
    "            y0_lidar_int = int(np.around(y0_lidar_float))\n",
    "                    \n",
    "        global_transform_vec = np.array([[np.cos(pose_angle)],[np.sin(pose_angle)]])\n",
    "        \n",
    "#          transform lidar to global frame for every reading, in meters\n",
    "\n",
    "\n",
    "        lidar_global_hit_locs_reduced = [np.dot(lidar_global_hit_locs_reduced[i],global_transform_vec) for i in range(len(lidar_global_hit_locs_reduced))]\n",
    "\n",
    "        # converts to array and squeeze extra dimension\n",
    "        lidar_global_hit_locs_reduced = np.asarray(lidar_global_hit_locs_reduced)\n",
    "#         print(lidar_global_hit_locs_reduced)\n",
    "#         print(lidar_global_hit_locs_reduced.shape)\n",
    "        lidar_global_hit_locs_reduced = np.squeeze(lidar_global_hit_locs_reduced)\n",
    "        \n",
    "        for reading in range(len(lidar_global_hit_locs_reduced)):\n",
    "            if lidar_data[time_step]['scan'][reading] < robot_self_dist and\\\n",
    "            (abs(180/np.pi*lidar_angle_vec[reading])> angle_reducer):\n",
    "                continue \n",
    "                    \n",
    "            # shift by encoder location AND convert to smaller unit\n",
    "            x1_lidar_float = np.squeeze(lidar_global_hit_locs_reduced[reading,0]) + x0_encoder_float \n",
    "            \n",
    "#             print(x1_lidar_float,'x1_lidar_float')\n",
    "            y1_lidar_float = np.squeeze(lidar_global_hit_locs_reduced[reading,1]) + y0_encoder_float\n",
    "\n",
    "\n",
    "            # convert all end point to int variables\n",
    "            x1_lidar_int = int(np.around(x1_lidar_float))\n",
    "            y1_lidar_int = int(np.around(y1_lidar_float))\n",
    "\n",
    "            ''' THIS IS DONE WITHIN SAME TIME STEP --> Origin has NOT CHANGED'''\n",
    "            if x1_lidar_int == x1_lidar_int_old and y1_lidar_int == y1_lidar_int_old:\n",
    "                continue\n",
    "\n",
    "            x1_lidar_int_old = np.copy(x1_lidar_int)\n",
    "            y1_lidar_int_old = np.copy(y1_lidar_int)\n",
    "            hit_list.append([x1_lidar_int_old,y1_lidar_int_old])\n",
    "            \n",
    "            # generate list of impacted points\n",
    "            '''Note that we use a (y0,x0,y1,x1 convention because of the way map_array is indexed (rows first))'''\n",
    "            points_impacted = list(bresenham(y0_lidar_int,x0_lidar_int, y1_lidar_int,x1_lidar_int ))\n",
    "            map_array = update_map(points_impacted,map_array)\n",
    "\n",
    "        if allow_print: \n",
    "            particle_plot_cnt += 1\n",
    "            \n",
    "            plt.figure(figsize=(15,15))\n",
    "            plt.imshow(map_array, origin=\"lower\")\n",
    "            plt.arrow(x0_encoder_float, y0_encoder_float,                      map_width*conv_factor/20*np.cos(pose_angle),                      map_width*conv_factor/20*np.sin(pose_angle),                      length_includes_head=True,    head_width=10, head_length=5)\n",
    "            hit_vec_verify = np.asarray(hit_list)\n",
    "            plt.scatter(hit_vec_verify[:,0],hit_vec_verify[:,1],c='r',alpha=1,s=1)\n",
    "            plt.scatter(x0_lidar_int, y0_lidar_int,c='k',alpha=1)\n",
    "            plt.show()\n",
    "            print(\"TIME STEP FOR PLOT PARTICLE ABOVE:\",time_step)\n",
    "        \n",
    "        if time_step % 40 == 0:\n",
    "            plot_array_list.append([map_array,[x0_encoder_float,y0_encoder_float,pose_angle,time_step]])\n",
    "            \n",
    "    map_plot_list.append(plot_array_list)\n",
    "    plt.close()\n",
    "\n",
    "    all_map_list.append(np.copy(map_array))\n",
    "    print(\"Total Execution Time: {0}\".format(time.time()-start) )       \n",
    "    \n",
    "    \n",
    "    fig = plt.figure(figsize=(15,15))\n",
    "    map_copy = np.copy(map_array)\n",
    "\n",
    "    extent=(-map_width/2,map_width/2,-map_width/2,map_width/2)\n",
    "    plt.imshow(map_copy,origin = 'lower', extent = extent,cmap='binary')\n",
    "    plt.xlabel('Robot Horizontal Location (meters)', fontsize=18)\n",
    "    plt.ylabel('Robot Vertical Location (meters)', fontsize=18)\n",
    "    plt.xticks(fontsize= 16)\n",
    "    plt.yticks(fontsize= 16) \n",
    "\n",
    "    fig.suptitle('SLAM Occupancy Grid for Map {0}'.format(map_list_train[idx].id), fontsize=20)\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(top=0.95)\n",
    "    \n",
    "    \n",
    "    fig.savefig('maps/map{0}_time_{1}.png'.format(map_list_train[idx].id,start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = 50\n",
    "bottom = 40\n",
    "left = 30\n",
    "right = 50\n",
    "\n",
    "ann_list = []\n",
    "\n",
    "def update(i):\n",
    "    \n",
    "    for j, a in enumerate(ann_list):\n",
    "        a.remove()\n",
    "    ann_list[:] = []\n",
    "    \n",
    "    print(str(i/len(map_current)*100)[:5],\"% Complete\")\n",
    "\n",
    "    im_normed = map_current[i][0]\n",
    "    quiver_pars = map_current[i][1][:2]\n",
    "    angle =  map_current[i][1][2]\n",
    "    ax.imshow(im_normed, origin= \"lower\",extent=extent)\n",
    "    t = map_current[i][1][3]\n",
    "    \n",
    "    \n",
    "    if i == 0:\n",
    "        rect = matplotlib.patches.Rectangle((right, bottom), left-right,top-bottom, angle=0.0,color=(1,1,1),)\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        texting = plt.annotate('T = '+str(int(t/40))+\" sec\",(.5*(left+right), .5*(top+bottom)),\\\n",
    "            ha='center',va='center', fontsize=14, color='red')\n",
    "        \n",
    "        ann_list.append(texting)\n",
    "        \n",
    "        \n",
    "        ax.scatter((quiver_pars[0]-x_center)/conv_factor,(quiver_pars[1]-y_center)/conv_factor,c='r',s=5)\n",
    "        return ax\n",
    "    \n",
    "    if i > 0:\n",
    "        rect = matplotlib.patches.Rectangle((right, bottom), left-right,top-bottom, angle=0.0,color=(1,1,1),zorder = 3+i)\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        texting = plt.annotate('T = '+str(int(t/40))+\" sec\",(.5*(left+right), .5*(top+bottom)),\\\n",
    "            ha='center',va='center', fontsize=14, color='red',zorder = 4+i)\n",
    "        \n",
    "        ann_list.append(texting)\n",
    "\n",
    "        \n",
    "        ax.scatter((quiver_pars[0]-x_center)/conv_factor,(quiver_pars[1]-y_center)/conv_factor,c='r',s=5)\n",
    "        return ax\n",
    "    \n",
    "    #     ax.set_title(\"Angle: {}*pi/10\".format(i), fontsize=20)\n",
    "#     ax.set_axis_off()\n",
    "\n",
    "for index, map_current in enumerate(map_plot_list):\n",
    "    extent=(-map_width/2,map_width/2,-map_width/2,map_width/2)\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    ax.set_title('Robot Location Versus Time for Map'+str(map_list_train[index].id))\n",
    "    ax.set_xlabel('Robot Horizontal Location (meters)', fontsize=18)\n",
    "    ax.set_ylabel('Robot Vertical Location (meters)', fontsize=18)\n",
    "#     quiver_matrix = np.asarray(map_current)[:,1]\n",
    "#     quiver_matrix = np.asarray([np.asarray(i) for i in quiver_matrix])\n",
    "#     print(quiver_matrix.shape,\"shape\")\n",
    "#     print(quiver_matrix)\n",
    "\n",
    "#     ax.set_xticks(fontsize= 16)\n",
    "#     ax.yticks(fontsize= 16) \n",
    "    print(\"Figure\",map_list_train[index].id)\n",
    "    anim = FuncAnimation(fig, update, frames=np.arange(0, len(map_current)), interval=300,repeat=True)\n",
    "    anim.save('gifs/path_gif_map'+str(map_list_train[index].id)+\".gif\", dpi=80, writer='imagemagick')\n",
    "    plt.close()\n",
    "    print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tar chvfz notebook.tar.gz 'maps'"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
